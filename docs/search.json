[
  {
    "objectID": "CoffeeRatings.html",
    "href": "CoffeeRatings.html",
    "title": "Coffee Ratings",
    "section": "",
    "text": "In this project I will be using data from Jo Hardin’s collection of “Tidy Tuesday” data sets.\n\nlibrary(tidyverse)\ndata &lt;- tidytuesdayR::tt_load('2020-07-07')\ncoffee_ratings &lt;- data[[\"coffee_ratings\"]]\n\nThe data Im using comes from the Coffee Quality Database posted by Buzzfeed Data Scientist James LeDoux. These data were collected from the Coffee Quality Institute’s review pages in January 2018.\n\ncoffee_ratings |&gt;\n  head(1)\n\n# A tibble: 1 × 43\n  total_cup_points species owner    country_of_origin farm_name lot_number mill \n             &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;             &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;\n1             90.6 Arabica metad p… Ethiopia          metad plc &lt;NA&gt;       meta…\n# ℹ 36 more variables: ico_number &lt;chr&gt;, company &lt;chr&gt;, altitude &lt;chr&gt;,\n#   region &lt;chr&gt;, producer &lt;chr&gt;, number_of_bags &lt;dbl&gt;, bag_weight &lt;chr&gt;,\n#   in_country_partner &lt;chr&gt;, harvest_year &lt;chr&gt;, grading_date &lt;chr&gt;,\n#   owner_1 &lt;chr&gt;, variety &lt;chr&gt;, processing_method &lt;chr&gt;, aroma &lt;dbl&gt;,\n#   flavor &lt;dbl&gt;, aftertaste &lt;dbl&gt;, acidity &lt;dbl&gt;, body &lt;dbl&gt;, balance &lt;dbl&gt;,\n#   uniformity &lt;dbl&gt;, clean_cup &lt;dbl&gt;, sweetness &lt;dbl&gt;, cupper_points &lt;dbl&gt;,\n#   moisture &lt;dbl&gt;, category_one_defects &lt;dbl&gt;, quakers &lt;dbl&gt;, color &lt;chr&gt;, …\n\n\nThe variable I am using is ‘total_cup_points’ which is the Total rating/points of a specific coffee on a scale from 0-100.\n\nnew_coffee_ratings &lt;- coffee_ratings |&gt;\n  group_by(country_of_origin) |&gt;\n  filter(!is.na(country_of_origin)) |&gt;\n  summarise(avg_score = mean(total_cup_points, na.rm = TRUE)) |&gt;\n  arrange(desc(avg_score))\n\nThis chunk takes the average “total_cup_points” for each country in the data set, and arranges those values in descending order. Now we can plot our findings using ggplot.\n\nggplot(new_coffee_ratings, aes(x = reorder(country_of_origin, avg_score), y = avg_score, fill = country_of_origin)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = 'Average \"Cup Points\" Score by Country of Origin',\n       x = \"Country of Origin\",\n       y = \"Average Score\") +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        axis.text = element_text(size = 7))\n\n\n\n\nAs we can see from the plot, Papa New Guinea has the highest Average Score in total cup points. We can also conclude that most total cup point scores are between 75 and 90, as all of the bars are relatively close in length."
  },
  {
    "objectID": "SimulatedSpotify.html#generate-listening-history",
    "href": "SimulatedSpotify.html#generate-listening-history",
    "title": "Simulated Spotify Wrapped",
    "section": "Generate Listening History",
    "text": "Generate Listening History\n\nwant to simulate a YEAR worth of listening history\n60,000 / 3.5 = 17,100\n\n\nmy_spotify_library &lt;- read.csv(\"~/paitynleigh.github.io/MyspotifyLibrary.csv\")\n\ncolnames(my_spotify_library)\n\n[1] \"Track.name\"    \"Artist.name\"   \"Album\"         \"Playlist.name\"\n[5] \"Type\"          \"ISRC\"          \"Spotify...id\" \n\n\n\n\nTool “Tune My Music” Used to collect playlist data https://www.tunemymusic.com/transfer"
  },
  {
    "objectID": "SimulatedSpotify.html#data-conclusions",
    "href": "SimulatedSpotify.html#data-conclusions",
    "title": "Simulated Spotify Wrapped",
    "section": "Data Conclusions",
    "text": "Data Conclusions\n\nRecreating Spotify Wrapped\nTop 5 Songs, Artists, and Albums\n\n\ntop_5 &lt;- function(data, colname){\n  data |&gt;\n    count({{colname}}, sort = TRUE)|&gt;\n    head(5)\n}\n\n\nCan use this same function for songs, artists, and albums"
  },
  {
    "objectID": "SimulatedSpotify.html#now-lets-see-1-iteration-of-simulated-listening-history",
    "href": "SimulatedSpotify.html#now-lets-see-1-iteration-of-simulated-listening-history",
    "title": "Simulated Spotify Wrapped",
    "section": "Now, Lets see 1 iteration of Simulated Listening History",
    "text": "Now, Lets see 1 iteration of Simulated Listening History\n\nfind_top_5 &lt;- function(simulated_year) {\n  top_songs &lt;- top_5(simulated_year, Track.name)\n  top_artists &lt;- top_5(simulated_year, Artist.name)\n  top_albums &lt;- top_5(simulated_year, Album)\n  \n  # labels each table \n  list(songs = top_songs, artists = top_artists, albums = top_albums)\n}\nfind_top_5(simulated_year(my_spotify_library))\n\n$songs\n         Track.name  n\n1            Nobody 33\n2              Hope 27\n3       Lay Me Down 26\n4               Why 25\n5 Come Away With Me 24\n\n$artists\n          Artist.name   n\n1              Mitski 478\n2        Lana Del Rey 269\n3 Panic! At The Disco 237\n4              Alex G 205\n5             TV Girl 193\n\n$albums\n                      Album   n\n1 Cry Baby (Deluxe Edition) 122\n2             Be the Cowboy 117\n3       Death of a Bachelor  92\n4        My Worlds Acoustic  80\n5                   MONTERO  78"
  },
  {
    "objectID": "SimulatedSpotify.html#mapping",
    "href": "SimulatedSpotify.html#mapping",
    "title": "Simulated Spotify Wrapped",
    "section": "Mapping",
    "text": "Mapping\n\nLets create multiple simulated years of listening\n\n\niters &lt;- 1000\n\nresults &lt;- map(1:iters, ~ {\n  simulated_year &lt;- simulated_year(my_spotify_library) \n  find_top_5(simulated_year) \n})\n\n\nWhat questions can mapping help us answer?"
  },
  {
    "objectID": "SimulatedSpotify.html#probability",
    "href": "SimulatedSpotify.html#probability",
    "title": "Simulated Spotify Wrapped",
    "section": "Probability",
    "text": "Probability\n\nHow likely is it that a specific artist will be in my top 5? Top 1??\n\n\ncheck_artist &lt;- function(top_artists, artist) {\n  in_top_5 &lt;- top_artists|&gt;\n    filter(Artist.name == artist) |&gt;\n    # Will return Boolean for this statement\n    nrow() &gt; 0  \n  \n  is_top_artist &lt;- top_artists |&gt;\n    pull(Artist.name) |&gt;\n    head(1) == artist \n  \n  list(\n    in_top_5 = in_top_5,\n    is_top_artist = is_top_artist\n  )\n}"
  },
  {
    "objectID": "SimulatedSpotify.html#cont.",
    "href": "SimulatedSpotify.html#cont.",
    "title": "Simulated Spotify Wrapped",
    "section": "cont.",
    "text": "cont.\n\nartist &lt;- \"TV Girl\"\n\nartist_occurrences &lt;- map(results, function(results) {\n\n  check_artist(pluck(results, \"artists\"), artist)\n})\n\ndata &lt;- map_dfr(artist_occurrences, ~ tibble(\n  in_top_5 = pluck(.x, \"in_top_5\"),\n  is_top_artist = pluck(.x, \"is_top_artist\")\n))\n\ndata|&gt;\n  head(1)\n\n# A tibble: 1 × 2\n  in_top_5 is_top_artist\n  &lt;lgl&gt;    &lt;lgl&gt;        \n1 FALSE    FALSE"
  },
  {
    "objectID": "SimulatedSpotify.html#conclusions",
    "href": "SimulatedSpotify.html#conclusions",
    "title": "Simulated Spotify Wrapped",
    "section": "Conclusions",
    "text": "Conclusions\n\nprob_top_5 &lt;- data |&gt;\n  select(in_top_5) |&gt;\n  summarise(mean = mean(in_top_5))|&gt;\n  pull(mean)\n\nprob_top_1 &lt;- data |&gt;\n  select(is_top_artist) |&gt;\n  summarise(mean = mean(is_top_artist))|&gt;\n  pull(mean)\n\nprob_top_5\n\n[1] 0.284\n\nprob_top_1\n\n[1] 0"
  },
  {
    "objectID": "Rodentdata.html",
    "href": "Rodentdata.html",
    "title": "Rodent Species",
    "section": "",
    "text": "In this project I will be using data from Jo Hardin’s collection of “Tidy Tuesday” data sets.\n\ndata &lt;- tidytuesdayR::tt_load('2023-05-02')\nspecies &lt;- data[[\"species\"]]\nlibrary(tidyverse)\n\nThe data I am using comes from the Portal Project.The Portal Project is a long-term ecological study being conducted near Portal, AZ. Since 1977, the site has been used to study the interactions among rodents, ants and plants and their respective responses to climate. This data pertains specifically to rodents.\n\nspecies |&gt;\n  head(1)\n\n# A tibble: 1 × 15\n  species scientificname  taxa   commonname     censustarget unidentified rodent\n  &lt;chr&gt;   &lt;chr&gt;           &lt;chr&gt;  &lt;chr&gt;                 &lt;dbl&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 BA      Baiomys taylori Rodent Northern pygm…            1            0      1\n# ℹ 8 more variables: granivore &lt;dbl&gt;, minhfl &lt;dbl&gt;, meanhfl &lt;dbl&gt;,\n#   maxhfl &lt;dbl&gt;, minwgt &lt;dbl&gt;, meanwgt &lt;dbl&gt;, maxwgt &lt;dbl&gt;, juvwgt &lt;dbl&gt;\n\n\nThe variables I will be using are ‘meanwgt’ and ‘juvwgt’. These are the average adult weight and the average juvenile weight of a given species, respectively.\n\nweight_data &lt;- species |&gt;\n  mutate(weight_difference = meanwgt - juvwgt) |&gt;\n  filter(!is.na(weight_difference)) |&gt;\n  select(commonname, weight_difference)\n\nI wanted to find the average amount of growth a given rodent experiences in its lifetime so I subtracted the juvenile weight from the mean weight of the species. We then get a table with the species’ common name and the “weight_difference” or the growth amount in the species life.\n\nlibrary(ggplot2)\nggplot(weight_data, aes(x = reorder(commonname, weight_difference), y = weight_difference, fill = weight_difference)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() + \n  scale_fill_gradient(low = \"pink\", high = \"orange\") +\n  labs(title = \"Average Lifetime Growth Across Rodent Species\",\n       x = \"Species Name\",\n       y = \"Growth Amount (g)\") +\n  theme_minimal()\n\n\n\n\nWe can conclude from the visual that the White-throated Woodrat experiences the largest amount of growth in its life. The growth amount measurement does not account for the size of the species meaning a better measurement for comparison would be the growth amount relative to the species average size."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Paityn Richardson",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "Project3.html",
    "href": "Project3.html",
    "title": "Poker Hands Probability",
    "section": "",
    "text": "In this project I plan to simulate the probability of being dealt a specific hand while playing poker. First I will simulate a deck of cards and then simulate the drawing of a 5 card hand with a function. Then I will use another function to check if the hand drawn at random matches a specific, special hand such as a full house, royal flush, etc.\nFirst, lets simulate a deck of cards\n\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(purrr)\n\n#use crossing function to associate each of the values with one of each of the suits \n\n\ndeck &lt;- crossing(value = c(2:10, \"J\", \"Q\", \"K\", \"A\"),\n\n                 suit = c(\"Spades\", \"Diamonds\", \"Clubs\", \"Hearts\"))\n\nNow, lets create a function that will deal a random hand of 5 cards from the deck.\n\ndeal_hand &lt;- function(deck){\n\n#pick random row numbers then index through deck data set to pick those specific rows \n\n  hand &lt;- deck[sample(nrow(deck), 5, replace = FALSE), ]\n\n  return(hand)\n\n}\n\ndeal_hand(deck)\n\n# A tibble: 5 × 2\n  value suit    \n  &lt;chr&gt; &lt;chr&gt;   \n1 9     Hearts  \n2 A     Diamonds\n3 A     Spades  \n4 8     Spades  \n5 K     Spades  \n\n\nNow, lets write a few functions that will check for special hands.\n\ncheck_straight &lt;- function(hand){\n\n  value_order &lt;- c(\"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"J\", \"Q\", \"K\", \n\n\"A\")\n\n  ordered_hand_values &lt;- match(hand$value, value_order)\n  \n  is_straight &lt;- all(diff(sort(ordered_hand_values)) == 1)\n\n  return(is_straight)\n\n}\n\ncheck_royal_flush &lt;- function(hand){\n\n  royal_values &lt;- c(\"10\", \"J\", \"Q\", \"K\", \"A\")\n\n  \n\n  is_flush &lt;- if_else(length(unique(hand$suit)) == 1, TRUE, FALSE)\n\n  is_royal &lt;- if_else(all(royal_values %in% hand$value), TRUE, FALSE)\n\n  return(is_flush && is_royal)\n\n}\n\ncheck_full_house &lt;- function(hand){\n\n  value_frequency &lt;- table(hand$value)\n\n  \n\n  #Gives True if there exactly 2 unique values and there are 3 repeats of the first value and 2 repeats of the second\n\n    if_else(length(value_frequency) == 2 &\n\n            all(value_frequency == c(3,2)), TRUE, FALSE)\n\n}\n\nNow, say this is a specific night at a casino and we want to know the probability that someone will be dealt a full house or a royal flush. Lets map these functions over numerous iterations in order to model this.\n\nset.seed(738)\niterations &lt;- 100000\n\nresults &lt;- \n  tibble(full_house = mean(map_lgl(1:iterations, ~check_full_house(deal_hand(deck)))),\n        royal_flush = mean(map_lgl(1:iterations, ~check_royal_flush(deal_hand(deck)))),\n        straight = mean(map_lgl(1:iterations, ~check_straight(deal_hand(deck)))))\n\nresults &lt;- results|&gt;\n  pivot_longer(cols = everything(), names_to = \"hand\", values_to = \"probability\")\n\nNow lets plot these results using a bar plot.\n\nggplot(results, aes(x = hand, y = probability, fill = hand)) +\n\n  geom_bar(stat = \"identity\") +\n\n  labs(title = \"Probabilities of Different Poker Hands\",\n\n       x = \"Poker Hand\",\n\n       y = \"Probability\") +\n\n  theme_minimal()\n\n\n\n\nAs we can see on the bar plot, a straight is the most common out of all the hands followed a full house and then a royal flush. With this information we can start to understand how common it is to be dealt certain hands in poker. This can be useful when trying to curb cheating attempts and or trying to place bets. This could also be helpful when designing online poker games and attempting to accurately replicate a real life game."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "hahahahhahha im evil lollllllowoahfbu3iebfuwbeiw",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "WAIDatabaseStudy.html",
    "href": "WAIDatabaseStudy.html",
    "title": "WAI Database Study",
    "section": "",
    "text": "library(RMariaDB)\nlibrary(tidyverse)\ncon_wai &lt;- dbConnect(\n  MariaDB(), host = \"scidb.smith.edu\",\n  user = \"waiuser\", password = \"smith_waiDB\", \n  dbname = \"wai\"\n)\nMeasurements &lt;- tbl(con_wai, \"Measurements\")\nPI_Info &lt;- tbl(con_wai, \"PI_Info\")\nSubjects &lt;- tbl(con_wai, \"Subjects\")\n\n# collect(Measurements)\n\nIn this project I plan to use the data from the Wideband Acoustic Immittance (WAI) Database hosted by Smith College. I plan to replicate a figure published in the Official Journal of the American Auditory Society by Susan Voss titled “Mean Absorbance from each Publication in the WAI Database” Voss(2020). The database contains 3 data sets: Measurements, PI_Info, and Subjects, which all have different information pertaining to the study.\nHere is the measurements table,\n\nDESCRIBE Measurements\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nIdentifier\nvarchar(50)\nNO\nPRI\nNA\n\n\n\nSubjectNumber\nint\nNO\nPRI\nNA\n\n\n\nSession\nint\nNO\nPRI\nNA\n\n\n\nEar\nvarchar(50)\nNO\nPRI\n\n\n\n\nInstrument\nvarchar(50)\nNO\nPRI\n\n\n\n\nAge\nfloat\nYES\n\nNA\n\n\n\nAgeCategory\nvarchar(50)\nYES\n\nNA\n\n\n\nEarStatus\nvarchar(50)\nYES\n\nNA\n\n\n\nTPP\nfloat\nYES\n\nNA\n\n\n\nAreaCanal\nfloat\nYES\n\nNA\n\n\n\n\n\n\nHere is the PI_Info table,\n\nDESCRIBE PI_Info\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nIdentifier\nvarchar(50)\nNO\nPRI\nNA\n\n\n\nYear\nint\nNO\n\nNA\n\n\n\nAuthors\ntext\nNO\n\nNA\n\n\n\nAuthorsShortList\ntext\nNO\n\nNA\n\n\n\nTitle\ntext\nNO\n\nNA\n\n\n\nJournal\ntext\nNO\n\nNA\n\n\n\nURL\ntext\nNO\n\nNA\n\n\n\nAbstract\ntext\nNO\n\nNA\n\n\n\nDataSubmitterName\ntext\nNO\n\nNA\n\n\n\nDataSubmitterEmail\ntext\nNO\n\nNA\n\n\n\n\n\n\nAnd here is the Subjects table,\n\nDESCRIBE Subjects\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nIdentifier\nvarchar(50)\nNO\nPRI\nNA\n\n\n\nSubjectNumber\nint\nNO\nPRI\nNA\n\n\n\nSessionTotal\nint\nNO\n\nNA\n\n\n\nAgeFirstMeasurement\nfloat\nYES\n\nNA\n\n\n\nAgeCategoryFirstMeasurement\nvarchar(50)\nYES\n\nNA\n\n\n\nSex\nvarchar(50)\nNO\n\nNA\n\n\n\nRace\nvarchar(50)\nNO\n\nNA\n\n\n\nEthnicity\nvarchar(50)\nNO\n\nNA\n\n\n\nLeftEarStatusFirstMeasurement\nvarchar(50)\nNO\n\nNA\n\n\n\nRightEarStatusFirstMeasurement\nvarchar(50)\nNO\n\nNA\n\n\n\n\n\n\nI started of by arranging the data using a SQL query.\n\n    SELECT \n        m.Frequency,\n        m.Identifier,\n        m.Instrument,\n        AVG(m.Absorbance) AS Mean_Absorption, \n        CONCAT(pi.AuthorsShortList, ' (', pi.Year, ')', ' N=', COUNT(DISTINCT CONCAT(m.SubjectNumber, '-', m.Ear)),'; ', m.Instrument) AS Label\n    FROM \n        Measurements m\n    JOIN \n        PI_Info pi\n        ON m.Identifier = pi.Identifier\n    WHERE \n        m.Identifier IN ('Abur_2014', 'Feeney_2017', 'Groon_2015', 'Lewis_2015', 'Lui_2008', 'Rosowski_2012', 'Shahnaz_2006', 'Shaver_2013', 'Sun_2016', 'Voss_1994', 'Voss_2010', 'Werner_2010') \n        AND m.Frequency BETWEEN 200 AND 8000\n    GROUP BY \n        m.Frequency, \n        m.Identifier, \n        m.Instrument\n    ORDER BY \n        m.Frequency, \n        m.Identifier, \n        m.Instrument\n\nIn this query, I grouped the data by Frequency, Identifier, and Instrument. In order to get the variable “AuthorShortList” I joined the PI_Info table and the Measurements table on SubjectNumber. I used Concat to make a label for each of the studies with the number of unique ears, the instrument used, and the year and author of the study. My goal was to match the figure created by Voss in 2022, so I filtered for only studies included in that specific plot.\n\nlibrary(ggplot2)\n\n\nggplot(new_table, aes(x = Frequency, y = Mean_Absorption, color = Label)) +\n  geom_line(size = .7, na.rm = TRUE) +  \n  scale_x_log10(name = \"Frequency (Hz)\",\n    breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000),\n    labels = c(\"200\", \"400\", \"600\", \"800\", \"1000\", \"2000\", \"4000\", \"6000\", \"8000\")) +\n  scale_y_continuous(\n    name = \"Mean Absorbance\",\n    limits = c(0, 1), \n    breaks = seq(0, 1, by = 0.1)  \n  ) +\n  labs(\n    title = \"Mean Absorbance From Each Publication in the WAI Database\",  \n    color = NULL \n  ) +\n  theme_minimal() + \n  theme(\n    axis.text = element_text(size = 5),\n    axis.title = element_text(size = 7),\n    plot.title = element_text(size = 9, face = \"bold\", hjust = .5),\n    legend.text = element_text(size = 5),\n    legend.position = c(0.28, 0.9),\n    legend.key.size = unit(0.1, \"cm\"),\n    legend.spacing = unit(0.01, \"cm\"),\n    legend.background = element_rect(color = \"black\", size = .2, fill = \"white\"),\n    aspect.ratio = 1\n  ) \n\n\n\n\nThis table shows the mean absorbance of frequencies ranging from 200 HZ to 8000 HZ in 12 different studies in the WAI database. As seen in the table, absorbance, on average, increases as frequency increases up until around 1000 HZ, when it drops off and begins to decrease again.\n\nSELECT \n    m.Frequency, \n    s.Sex, \n    AVG(m.Absorbance) AS Avg_Absorbance\nFROM \n    Measurements m\nJOIN \n    Subjects s\nON \n    m.SubjectNumber = s.SubjectNumber\nWHERE \n    m.Identifier = 'Aithal_2015'\n    AND s.Sex != 'Unknown'\n    AND m.Frequency BETWEEN 200 AND 8000\nGROUP BY \n    m.Frequency, s.Sex\nORDER BY \n    m.Frequency, s.Sex;\n\nNow with this SQL query, I am doing a similar thing to the last, except now I am also grouping by “sex” which required me to join the table “Subjects” also in the WAI database. For this table i chose to focus on 1 study in the WAI data base, this being A Comparison With High-Frequency Tympanometry, Automated Brainstem Response, and Transient Evoked and Distortion Product Otoacoustic Emissions(Aithal 2015).\n\ndbDisconnect(con_wai, shutdown = TRUE)\n\n\nlibrary(ggplot2)\n\nggplot(new_table4, aes(x = Frequency, y = Avg_Absorbance, color = Sex)) +\n  geom_line(size = 0.7, na.rm = TRUE) +\n  scale_x_log10(name = \"Frequency (Hz)\",\n    breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000),\n    labels = c(\"200\", \"400\", \"600\", \"800\", \"1000\", \"2000\", \"4000\", \"6000\", \"8000\")) +\n  scale_y_continuous(\n    name = \"Average Absorbance\",\n    limits = c(0, 1), \n    breaks = seq(0, 1, by = 0.1)\n  ) +\n  labs(\n    title = \"Average Absorbance by Frequency and Gender(Aithal 2015)\",\n    color = NULL \n  ) +\n  theme_minimal() +\n  theme(\n    axis.text = element_text(size = 8),\n    axis.title = element_text(size = 12),\n    plot.title = element_text(size = 16, face = \"bold\"),\n    legend.text = element_text(size = 8),\n    legend.position = c(0.1, 0.9),\n    legend.box.background = element_rect(color = \"black\", fill = \"white\", size = 0.5) \n  ) +\n  coord_cartesian(xlim = c(200, 8000)) \n\n\n\n\nAgain, this plot shows the average absorbance over frequencies ranging from 200 to 8000. The 2 lines in the plot represent Male and Female tested subjects. As shown by the plot, there is almost no difference in the mean absorbance rates based on gender."
  },
  {
    "objectID": "FinalProject2.html",
    "href": "FinalProject2.html",
    "title": "Spotify Patterns",
    "section": "",
    "text": "library(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\ndataset_2 &lt;- read_csv(\"~/FODS-project2/dataset 2.csv\")\nspotify &lt;- dataset_2\n\nThe dataset that I am using contains a large array of Spotify tracks, spanning over 100 genres and almost 75000 unique tracks. The data was published on Kaggle by Maharshi Pandya in 2022. It is titled “Spotify Tracks Dataset”\nThis is a preview of the data and its variables.\n\nspotify |&gt;\n  head(1)\n\n# A tibble: 1 × 21\n   ...1 track_id   artists album_name track_name popularity duration_ms explicit\n  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;       &lt;dbl&gt; &lt;lgl&gt;   \n1     0 5SuOikwiR… Gen Ho… Comedy     Comedy             73      230666 FALSE   \n# ℹ 13 more variables: danceability &lt;dbl&gt;, energy &lt;dbl&gt;, key &lt;dbl&gt;,\n#   loudness &lt;dbl&gt;, mode &lt;dbl&gt;, speechiness &lt;dbl&gt;, acousticness &lt;dbl&gt;,\n#   instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;, valence &lt;dbl&gt;, tempo &lt;dbl&gt;,\n#   time_signature &lt;dbl&gt;, track_genre &lt;chr&gt;\n\n\nTo begin, I want to see find the frequency that each artist and each album appears.\n\nartist_freq &lt;- spotify |&gt;\n  count(artists, sort = TRUE)\n\nalbum_freq &lt;- spotify|&gt;\n  count(album_name, sort = TRUE)\n\nNext, we want to see the top 10 most frequently appearing artists and top 10 most frequently appearing albums.\n\ntop10_artists &lt;- head(artist_freq, 10)\n\ntop10_albums &lt;- head(album_freq, 10)\n\nFinally, we can plot these findings on bar charts.\n\nggplot(top10_artists, aes(x = reorder(artists, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"green\") +\n  coord_flip() +\n  labs(title = \"Top 10 Most Frequent Artists\", x = \"Artist\", y = \"Count\")\n\n\n\nggplot(top10_albums, aes(x = reorder(album_name, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"green\") +\n  coord_flip() +\n  labs(title = \"Top 10 Most Frequent Albums\", x = \"Album Name\", y = \"Count\")\n\n\n\n\nNext, lets look at the song titles. The data set contains a column “popularity_score” that is calculated by spotify software. We want to to see how having certain “buzz words” in a song title affects the songs popularity score.\nTo do this lets first find the words that appear the most frequently in the song titles.\n\nsong_titles &lt;- tolower(spotify$track_name)\n  words &lt;- str_extract_all(song_titles, \"\\\\b\\\\w+(?:'\\\\w+)?\\\\b\")|&gt;\n  unlist()|&gt;\n  as.data.frame()|&gt;\n  setNames(\"word\")|&gt;\n  count(word, sort = TRUE)|&gt;\n  \n    \n  #filter for articles and pronouns\n  filter(!(word %in% \n             c(\"the\", \"a\", \"of\", \"and\", \"you\", \"me\", \"i\", \"to\", \"my\" )))|&gt;\n  arrange(desc(n))|&gt;\n  head(50)\n\nIn the data set, track names contain descriptions for the track such as “live”, “remix”, or “remaster”, which are not necessarily words included in the song title, though they come up as being the most common words. Still, we can see from this table that words like “love”, “Christmas”, “time” and “night” are among the top 50 most common words in song titles.\nWith this, we can look to see if having one of these common “buzz words” in a song title contributes to the popularity score of a given song.\n\nspotify &lt;- spotify |&gt;\n  filter(!is.na(track_name)) |&gt;\n  mutate(has_love = ifelse(str_detect(track_name, regex(\"\\\\blove\\\\b\", ignore_case = TRUE)), \"Yes\", \"No\"))\n\npopularity &lt;- spotify |&gt;\n  group_by(has_love) |&gt;\n  summarise(avg_popularity = mean(popularity, na.rm = TRUE))\n\npopularity |&gt;\nggplot(aes(x = has_love, y = avg_popularity, fill = has_love)) +\n  geom_bar(stat = \"identity\", width = 0.8) +  \n  labs(title = \"Average Popularity of Songs with vs. without 'Love' in Title\",\n       x = \"Contains 'Love'\",\n       y = \"Average Popularity Score\") +\n  scale_fill_manual(values = c(\"Yes\" = \"magenta\", \"No\" = \"lightblue\")) +\n  theme_minimal()\n\n\n\n\nFrom this, we can see songs that contain the word “love” tend to have a higher popularity score on average. Now lest do this with a few of the other most common words.\n\nspotify &lt;- spotify |&gt;\n  filter(!is.na(track_name)) |&gt;\n  mutate(has_christmas = ifelse(str_detect(track_name, regex(\"\\\\bchristmas\\\\b\", ignore_case = TRUE)), \"Yes\", \"No\"))\n\npopularity &lt;- spotify |&gt;\n  group_by(has_christmas) |&gt;\n  summarise(avg_popularity = mean(popularity, na.rm = TRUE))\n\npopularity |&gt;\nggplot(aes(x = has_christmas, y = avg_popularity, fill = has_christmas)) +\n  geom_bar(stat = \"identity\", width = 0.8) +  \n  labs(title = \"Average Popularity of Songs with vs. without 'Christmas' in Title\",\n       x = \"Contains 'Christmas'\",\n       y = \"Average Popularity Score\") +\n  scale_fill_manual(values = c(\"Yes\" = \"red\", \"No\" = \"chartreuse\")) +\n  theme_minimal()\n\n\n\n\n\nspotify &lt;- spotify |&gt;\n  filter(!is.na(track_name)) |&gt;\n  mutate(has_night = ifelse(str_detect(track_name, regex(\"\\\\bnight\\\\b\", ignore_case = TRUE)), \"Yes\", \"No\"))\n\npopularity &lt;- spotify |&gt;\n  group_by(has_night) |&gt;\n  summarise(avg_popularity = mean(popularity, na.rm = TRUE))\n\npopularity |&gt;\nggplot(aes(x = has_night, y = avg_popularity, fill = has_night)) +\n  geom_bar(stat = \"identity\", width = 0.8) +  \n  labs(title = \"Average Popularity of Songs with vs. without 'Night' in Title\",\n       x = \"Contains 'Night'\",\n       y = \"Average Popularity Score\") +\n  scale_fill_manual(values = c(\"Yes\" = \"darkgoldenrod4\", \"No\" = \"burlywood4\")) +\n  theme_minimal()\n\n\n\n\nAs we can see from these charts, song names that contain the word “Christmas” have a much lower popularity score overall, while song names that contain the word “Night”, have just a slightly lower popularity score on average.\nUsing these findings, we can begin to look at the correlation between a songs title and its popularity, maybe leading to findings about how to title songs and what makes the BEST song title."
  },
  {
    "objectID": "ResearchBlog.html",
    "href": "ResearchBlog.html",
    "title": "Daily Research Blog 2025",
    "section": "",
    "text": "Read through the beginning half of PAYS curriculum and took notes in a google doc.\nFinish reading through the rest and take notes on the structure of problems.\nI’m still a little confused about the physical logistics of computers in the classroom. It looks like the students mostly work in groups already, so maybe shared devices with a server connection would work, so every student doesn’t need to download on their own device?"
  },
  {
    "objectID": "ResearchBlog.html#date-may-19-2025",
    "href": "ResearchBlog.html#date-may-19-2025",
    "title": "Daily Research Blog 2025",
    "section": "",
    "text": "Read through the beginning half of PAYS curriculum and took notes in a google doc.\nFinish reading through the rest and take notes on the structure of problems.\nI’m still a little confused about the physical logistics of computers in the classroom. It looks like the students mostly work in groups already, so maybe shared devices with a server connection would work, so every student doesn’t need to download on their own device?"
  },
  {
    "objectID": "Example Problem Integration.html#modeling-growth-and-decay",
    "href": "Example Problem Integration.html#modeling-growth-and-decay",
    "title": "Example Problem Integration",
    "section": "Modeling Growth and Decay",
    "text": "Modeling Growth and Decay"
  },
  {
    "objectID": "ResearchBlog.html#date-may-20th-2025",
    "href": "ResearchBlog.html#date-may-20th-2025",
    "title": "Daily Research Blog 2025",
    "section": "Date: May 20th 2025",
    "text": "Date: May 20th 2025\n\nContinued Reading through Pays curriculum. Created an infographic for teaching the difference between Quantitative and Categorical variables. Re-wrote a few problems what used excel in r.\nFind more problems that cant be shifted from excel to r usage.\nHow is material typically taught during the program. Is it similar to a college lecture with slideshow presentation? I wanted to start thinking about teaching materials for information but i realized i not sure how material is usually taught."
  }
]